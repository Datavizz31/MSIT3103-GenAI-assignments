{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8228eef2",
   "metadata": {},
   "source": [
    "# Simple Generative AI Model\n",
    "\n",
    "This notebook demonstrates a complete generative AI model implementation in a single Python file. The model uses Hugging Face Transformers library to create a text generation system that runs on CPU.\n",
    "\n",
    "## Overview\n",
    "- Import required libraries\n",
    "- Load and configure a pre-trained GPT-2 model\n",
    "- Create a simple text generation function\n",
    "- Test the model with sample inputs\n",
    "- Measure performance metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecff23b6",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries\n",
    "\n",
    "First, we import all the necessary libraries for our generative AI model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab3af98b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lalithyaalapati/Cloud_deploy/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/Users/lalithyaalapati/Cloud_deploy/.venv/lib/python3.12/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/Users/lalithyaalapati/Cloud_deploy/.venv/lib/python3.12/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x10ef769f0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import necessary libraries for the generative AI model\n",
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Check if GPU is available, otherwise use CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Set random seed for reproducible results\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a431ae",
   "metadata": {},
   "source": [
    "## 2. Define the Complete Generative AI Model\n",
    "\n",
    "This section contains the complete model implementation in a single Python class with all necessary methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d97d2ef2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Simple Generative AI Model...\n",
      "Loading model: gpt2\n",
      "Model loaded successfully on cpu\n"
     ]
    }
   ],
   "source": [
    "class SimpleGenerativeAI:\n",
    "    \"\"\"\n",
    "    A simple generative AI model using GPT-2 for text generation.\n",
    "    This class handles model loading, text generation, and performance monitoring.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_name=\"gpt2\"):\n",
    "        \"\"\"\n",
    "        Initialize the generative AI model.\n",
    "        \n",
    "        Args:\n",
    "            model_name (str): Name of the pre-trained model to use\n",
    "        \"\"\"\n",
    "        print(f\"Loading model: {model_name}\")\n",
    "        \n",
    "        # Load the pre-trained tokenizer\n",
    "        # Tokenizer converts text to tokens that the model can understand\n",
    "        self.tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "        \n",
    "        # Add padding token if it doesn't exist\n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        \n",
    "        # Load the pre-trained model\n",
    "        # This is the actual neural network that generates text\n",
    "        self.model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "        \n",
    "        # Move model to the appropriate device (CPU or GPU)\n",
    "        self.model.to(device)\n",
    "        \n",
    "        # Set model to evaluation mode for inference\n",
    "        self.model.eval()\n",
    "        \n",
    "        print(f\"Model loaded successfully on {device}\")\n",
    "    \n",
    "    def generate_text(self, prompt, max_length=100, temperature=0.7, num_return_sequences=1):\n",
    "        \"\"\"\n",
    "        Generate text based on a given prompt.\n",
    "        \n",
    "        Args:\n",
    "            prompt (str): Input text to continue\n",
    "            max_length (int): Maximum length of generated text\n",
    "            temperature (float): Controls randomness (lower = more deterministic)\n",
    "            num_return_sequences (int): Number of different outputs to generate\n",
    "        \n",
    "        Returns:\n",
    "            list: Generated text sequences\n",
    "        \"\"\"\n",
    "        # Record start time for performance measurement\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Convert text prompt to tokens\n",
    "        # Tokens are numerical representations that the model processes\n",
    "        input_ids = self.tokenizer.encode(prompt, return_tensors='pt').to(device)\n",
    "        \n",
    "        # Generate text using the model\n",
    "        with torch.no_grad():  # Disable gradient computation for faster inference\n",
    "            outputs = self.model.generate(\n",
    "                input_ids,\n",
    "                max_length=max_length,\n",
    "                temperature=temperature,\n",
    "                num_return_sequences=num_return_sequences,\n",
    "                do_sample=True,  # Enable sampling for more creative outputs\n",
    "                pad_token_id=self.tokenizer.eos_token_id,\n",
    "                no_repeat_ngram_size=2  # Prevent repetitive phrases\n",
    "            )\n",
    "        \n",
    "        # Convert generated tokens back to text\n",
    "        generated_texts = []\n",
    "        for output in outputs:\n",
    "            text = self.tokenizer.decode(output, skip_special_tokens=True)\n",
    "            generated_texts.append(text)\n",
    "        \n",
    "        # Calculate inference time\n",
    "        inference_time = time.time() - start_time\n",
    "        \n",
    "        # Print performance metrics\n",
    "        print(f\"Generation completed in {inference_time:.2f} seconds\")\n",
    "        print(f\"Generated {len(generated_texts)} sequence(s)\")\n",
    "        \n",
    "        return generated_texts\n",
    "    \n",
    "    def get_model_info(self):\n",
    "        \"\"\"\n",
    "        Get information about the loaded model.\n",
    "        \n",
    "        Returns:\n",
    "            dict: Model information including parameters and configuration\n",
    "        \"\"\"\n",
    "        # Count the number of parameters in the model\n",
    "        total_params = sum(p.numel() for p in self.model.parameters())\n",
    "        trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)\n",
    "        \n",
    "        info = {\n",
    "            \"model_name\": self.model.config.name_or_path,\n",
    "            \"total_parameters\": total_params,\n",
    "            \"trainable_parameters\": trainable_params,\n",
    "            \"vocab_size\": self.tokenizer.vocab_size,\n",
    "            \"device\": str(device)\n",
    "        }\n",
    "        \n",
    "        return info\n",
    "\n",
    "# Create an instance of our generative AI model\n",
    "print(\"Initializing Simple Generative AI Model...\")\n",
    "gen_ai = SimpleGenerativeAI(\"gpt2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f6bd6b",
   "metadata": {},
   "source": [
    "## 3. Display Model Information\n",
    "\n",
    "Let's examine the details of our loaded model to understand its capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e524ad5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Information:\n",
      "--------------------------------------------------\n",
      "Model Name: gpt2\n",
      "Total Parameters: 124,439,808\n",
      "Trainable Parameters: 124,439,808\n",
      "Vocab Size: 50257\n",
      "Device: cpu\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Get and display model information\n",
    "model_info = gen_ai.get_model_info()\n",
    "\n",
    "print(\"Model Information:\")\n",
    "print(\"-\" * 50)\n",
    "for key, value in model_info.items():\n",
    "    if \"parameters\" in key:\n",
    "        # Format large numbers for better readability\n",
    "        print(f\"{key.replace('_', ' ').title()}: {value:,}\")\n",
    "    else:\n",
    "        print(f\"{key.replace('_', ' ').title()}: {value}\")\n",
    "\n",
    "print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cca4afb",
   "metadata": {},
   "source": [
    "## 4. Test the Model with Sample Inputs\n",
    "\n",
    "Now let's test our generative AI model with various prompts to see how it performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3d9cdb00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Case 1: Story Generation\n",
      "============================================================\n",
      "Generation completed in 3.44 seconds\n",
      "Generated 1 sequence(s)\n",
      "Prompt: Once upon a time in a distant galaxy\n",
      "Generated: Once upon a time in a distant galaxy, an alien race had been born, but it was the only one that was able to create a living being. The race was called the Humanoid race, and they were the first human race to be born in the galaxy.\n",
      "\n",
      "The Humanoids were a race of beings who had evolved into the humanoid form. They were known as the Humans,\n",
      "\n",
      "============================================================\n",
      "\n",
      "Test Case 2: Technical Content\n",
      "============================================================\n",
      "Generation completed in 1.27 seconds\n",
      "Generated 1 sequence(s)\n",
      "Prompt: Artificial intelligence is\n",
      "Generated: Artificial intelligence is a new field of research that has been in the works for a while now, but it has now been demonstrated in a very early stage of development in an attempt to understand how it works.\n",
      "\n",
      "The researchers say that the new technology could be used to help people with autism, and that it could also help them to better understand\n",
      "\n",
      "============================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test Case 1: Simple story beginning\n",
    "print(\"Test Case 1: Story Generation\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "prompt1 = \"Once upon a time in a distant galaxy\"\n",
    "generated_text1 = gen_ai.generate_text(\n",
    "    prompt=prompt1,\n",
    "    max_length=80,\n",
    "    temperature=0.5,\n",
    "    num_return_sequences=1\n",
    ")\n",
    "\n",
    "print(f\"Prompt: {prompt1}\")\n",
    "print(f\"Generated: {generated_text1[0]}\")\n",
    "print(\"\\n\" + \"=\" * 60 + \"\\n\")\n",
    "\n",
    "# Test Case 2: Technical explanation\n",
    "print(\"Test Case 2: Technical Content\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "prompt2 = \"Artificial intelligence is\"\n",
    "generated_text2 = gen_ai.generate_text(\n",
    "    prompt=prompt2,\n",
    "    max_length=70,\n",
    "    temperature=0.1,  # Lower temperature for more focused output\n",
    "    num_return_sequences=1\n",
    ")\n",
    "\n",
    "print(f\"Prompt: {prompt2}\")\n",
    "print(f\"Generated: {generated_text2[0]}\")\n",
    "print(\"\\n\" + \"=\" * 60 + \"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c191fd",
   "metadata": {},
   "source": [
    "## 5. Performance Analysis and Benchmarking\n",
    "\n",
    "Let's analyze the performance of our model by testing different parameters and measuring execution time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb2b75df",
   "metadata": {},
   "source": [
    "## 6. Interactive Testing\n",
    "\n",
    "Try your own prompts with the model. Modify the prompt variable below to test different inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c83751bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing custom prompt: 'Machine learning is is important skill in this job market'\n",
      "============================================================\n",
      "Generation completed in 1.36 seconds\n",
      "Generated 1 sequence(s)\n",
      "Input: Machine learning is is important skill in this job market\n",
      "Output: Machine learning is is important skill in this job market, and many of the jobs available to those who don't have a diploma are not a good fit for the job.\n",
      "\n",
      "Here are some resources to help you learn how to apply for these jobs:\n",
      "...\n",
      "\n",
      "\n",
      "============================================================\n",
      "Model testing completed successfully!\n",
      "The generative AI model is working and ready for deployment.\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Interactive testing - modify this prompt to test your own inputs\n",
    "custom_prompt = \"Machine learning is is important skill in this job market\"\n",
    "\n",
    "print(f\"Testing custom prompt: '{custom_prompt}'\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Generate text with custom settings\n",
    "custom_result = gen_ai.generate_text(\n",
    "    prompt=custom_prompt,\n",
    "    max_length=100,          # Adjust length as needed\n",
    "    temperature=0.5,         # Adjust creativity (0.1 = conservative, 1.0 = creative)\n",
    "    num_return_sequences=1   # Number of different outputs\n",
    ")\n",
    "\n",
    "print(f\"Input: {custom_prompt}\")\n",
    "print(f\"Output: {custom_result[0]}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Model testing completed successfully!\")\n",
    "print(\"The generative AI model is working and ready for deployment.\")\n",
    "print(\"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
